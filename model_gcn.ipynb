{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "model_gcn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:16:38.355219Z",
          "iopub.execute_input": "2021-07-21T16:16:38.355516Z",
          "iopub.status.idle": "2021-07-21T16:16:44.653003Z",
          "shell.execute_reply.started": "2021-07-21T16:16:38.355486Z",
          "shell.execute_reply": "2021-07-21T16:16:44.651986Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LwZ3-gs4dg5",
        "outputId": "f0b4e780-0227-4609-eb11-0d00de29e7e9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re, json, itertools\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "!pip install openpyxl\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:16:44.656182Z",
          "iopub.execute_input": "2021-07-21T16:16:44.656496Z",
          "iopub.status.idle": "2021-07-21T16:17:01.985866Z",
          "shell.execute_reply.started": "2021-07-21T16:16:44.656463Z",
          "shell.execute_reply": "2021-07-21T16:17:01.984898Z"
        },
        "trusted": true,
        "id": "hSV4DIZn4dg9"
      },
      "source": [
        "df = pd.read_excel('posts.xlsx')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:01.987330Z",
          "iopub.execute_input": "2021-07-21T16:17:01.987698Z",
          "iopub.status.idle": "2021-07-21T16:17:01.995541Z",
          "shell.execute_reply.started": "2021-07-21T16:17:01.987660Z",
          "shell.execute_reply": "2021-07-21T16:17:01.994595Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFUG5BkL4dg-",
        "outputId": "d04e5ae4-c3f7-4849-fdc6-2df2722138e5"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39362, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:03.949231Z",
          "iopub.execute_input": "2021-07-21T16:17:03.949548Z",
          "iopub.status.idle": "2021-07-21T16:17:04.033991Z",
          "shell.execute_reply.started": "2021-07-21T16:17:03.949518Z",
          "shell.execute_reply": "2021-07-21T16:17:04.033180Z"
        },
        "trusted": true,
        "id": "HWJBsfwe4dg_"
      },
      "source": [
        "df = df[['Attribute:Body', 'Attribute:Tags']]\n",
        "df.columns = ['Body', 'Tags']\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "df = df.iloc[:10000, :]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:05.001221Z",
          "iopub.execute_input": "2021-07-21T16:17:05.001548Z",
          "iopub.status.idle": "2021-07-21T16:17:05.016491Z",
          "shell.execute_reply.started": "2021-07-21T16:17:05.001519Z",
          "shell.execute_reply": "2021-07-21T16:17:05.015556Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "G2Y7GZRa4dg_",
        "outputId": "3e1d8f1a-4f89-4f88-f4bd-e326d5bfb4dd"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;p&gt;I often hear about subatomic particles havi...</td>\n",
              "      <td>&lt;quantum-mechanics&gt;&lt;particle-physics&gt;&lt;angular-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;p&gt;How would you explain string theory to non-...</td>\n",
              "      <td>&lt;string-theory&gt;&lt;education&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;p&gt;This is a question that has been posted at ...</td>\n",
              "      <td>&lt;particle-physics&gt;&lt;group-theory&gt;&lt;representatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;p&gt;What are the main problems that we need to ...</td>\n",
              "      <td>&lt;quantum-mechanics&gt;&lt;quantum-interpretations&gt;&lt;h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;p&gt;Hamilton's principle states that a dynamic ...</td>\n",
              "      <td>&lt;lagrangian-formalism&gt;&lt;variational-principle&gt;&lt;...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Body                                               Tags\n",
              "0  <p>I often hear about subatomic particles havi...  <quantum-mechanics><particle-physics><angular-...\n",
              "1  <p>How would you explain string theory to non-...                         <string-theory><education>\n",
              "2  <p>This is a question that has been posted at ...  <particle-physics><group-theory><representatio...\n",
              "3  <p>What are the main problems that we need to ...  <quantum-mechanics><quantum-interpretations><h...\n",
              "4  <p>Hamilton's principle states that a dynamic ...  <lagrangian-formalism><variational-principle><..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:05.224713Z",
          "iopub.execute_input": "2021-07-21T16:17:05.224970Z",
          "iopub.status.idle": "2021-07-21T16:17:05.232359Z",
          "shell.execute_reply.started": "2021-07-21T16:17:05.224945Z",
          "shell.execute_reply": "2021-07-21T16:17:05.231398Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dh8tUr-4dg_",
        "outputId": "f0f3f73b-28ed-4317-e3de-f11c493905fb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:05.973212Z",
          "iopub.execute_input": "2021-07-21T16:17:05.973537Z",
          "iopub.status.idle": "2021-07-21T16:17:15.913007Z",
          "shell.execute_reply.started": "2021-07-21T16:17:05.973505Z",
          "shell.execute_reply": "2021-07-21T16:17:15.912149Z"
        },
        "trusted": true,
        "id": "2S29rqQV4dhA"
      },
      "source": [
        "#preprocessing the Body\n",
        "def clean_text(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    sentence = re.sub(cleanr, '', sentence)\n",
        "    pattern = re.compile(r'[^a-z]+')\n",
        "    sentence = sentence.lower()\n",
        "    sentence = pattern.sub(' ', sentence).strip()\n",
        "    word_list = word_tokenize(sentence)\n",
        "#     stopwords_list = set(stopwords.words('english'))\n",
        "#     word_list = [word for word in word_list if word not in stopwords_list]\n",
        "#     word_list = [word for word in word_list if len(word)>1]\n",
        "#     ps = PorterStemmer()\n",
        "#     word_list = [ps.stem(word) for word in word_list]\n",
        "    sentence = ' '.join(word_list)\n",
        "    return sentence\n",
        "df['Body'] = df['Body'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:15.914427Z",
          "iopub.execute_input": "2021-07-21T16:17:15.914805Z",
          "iopub.status.idle": "2021-07-21T16:17:15.925050Z",
          "shell.execute_reply.started": "2021-07-21T16:17:15.914768Z",
          "shell.execute_reply": "2021-07-21T16:17:15.923895Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "U-Fi8eYW4dhA",
        "outputId": "f7e1128b-aa39-4431-c465-7cbcfe4471b6"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i often hear about subatomic particles having ...</td>\n",
              "      <td>&lt;quantum-mechanics&gt;&lt;particle-physics&gt;&lt;angular-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how would you explain string theory to non phy...</td>\n",
              "      <td>&lt;string-theory&gt;&lt;education&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is a question that has been posted at man...</td>\n",
              "      <td>&lt;particle-physics&gt;&lt;group-theory&gt;&lt;representatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the main problems that we need to sol...</td>\n",
              "      <td>&lt;quantum-mechanics&gt;&lt;quantum-interpretations&gt;&lt;h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hamilton s principle states that a dynamic sys...</td>\n",
              "      <td>&lt;lagrangian-formalism&gt;&lt;variational-principle&gt;&lt;...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Body                                               Tags\n",
              "0  i often hear about subatomic particles having ...  <quantum-mechanics><particle-physics><angular-...\n",
              "1  how would you explain string theory to non phy...                         <string-theory><education>\n",
              "2  this is a question that has been posted at man...  <particle-physics><group-theory><representatio...\n",
              "3  what are the main problems that we need to sol...  <quantum-mechanics><quantum-interpretations><h...\n",
              "4  hamilton s principle states that a dynamic sys...  <lagrangian-formalism><variational-principle><..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:20.385753Z",
          "iopub.execute_input": "2021-07-21T16:17:20.386098Z",
          "iopub.status.idle": "2021-07-21T16:17:26.151163Z",
          "shell.execute_reply.started": "2021-07-21T16:17:20.386071Z",
          "shell.execute_reply": "2021-07-21T16:17:26.150356Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "JYz_Luq44dhB",
        "outputId": "b075a9d0-a8d9-45e5-cd9d-ae9d81e2b0b5"
      },
      "source": [
        "#creating one-hot representation for all the tags\n",
        "df_tags = pd.DataFrame(np.zeros((df.shape[0], len_tags), dtype=int), columns=tags)\n",
        "for i in range(len_rows):\n",
        "    for x in df.Tags[i].split():\n",
        "        df_tags[x][i] = 1\n",
        "df = pd.concat([df, df_tags], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "      <th>1pi-effective-action</th>\n",
              "      <th>absolute-units</th>\n",
              "      <th>absorption</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>accelerator-physics</th>\n",
              "      <th>accretion-disk</th>\n",
              "      <th>acoustics</th>\n",
              "      <th>action</th>\n",
              "      <th>adiabatic</th>\n",
              "      <th>adm-formalism</th>\n",
              "      <th>ads-cft</th>\n",
              "      <th>aerodynamics</th>\n",
              "      <th>aether</th>\n",
              "      <th>affine-lie-algebra</th>\n",
              "      <th>air</th>\n",
              "      <th>aircraft</th>\n",
              "      <th>algebraic-geometry</th>\n",
              "      <th>algebraic-topology</th>\n",
              "      <th>algorithms</th>\n",
              "      <th>amorphous-solids</th>\n",
              "      <th>analyticity</th>\n",
              "      <th>angular-momentum</th>\n",
              "      <th>angular-velocity</th>\n",
              "      <th>antennas</th>\n",
              "      <th>anthropic-principle</th>\n",
              "      <th>anti-de-sitter-spacetime</th>\n",
              "      <th>anticommutator</th>\n",
              "      <th>antimatter</th>\n",
              "      <th>anyons</th>\n",
              "      <th>applied-physics</th>\n",
              "      <th>approximations</th>\n",
              "      <th>arrow-of-time</th>\n",
              "      <th>asteroids</th>\n",
              "      <th>astrometrics</th>\n",
              "      <th>astronomy</th>\n",
              "      <th>astrophotography</th>\n",
              "      <th>astrophysics</th>\n",
              "      <th>asymptotics</th>\n",
              "      <th>...</th>\n",
              "      <th>variational-calculus</th>\n",
              "      <th>variational-principle</th>\n",
              "      <th>vector-fields</th>\n",
              "      <th>vectors</th>\n",
              "      <th>velocity</th>\n",
              "      <th>vibrations</th>\n",
              "      <th>virial-theorem</th>\n",
              "      <th>virtual-particles</th>\n",
              "      <th>viscosity</th>\n",
              "      <th>visible-light</th>\n",
              "      <th>vision</th>\n",
              "      <th>visualization</th>\n",
              "      <th>voltage</th>\n",
              "      <th>volume</th>\n",
              "      <th>vortex</th>\n",
              "      <th>warp-drives</th>\n",
              "      <th>water</th>\n",
              "      <th>wave-particle-duality</th>\n",
              "      <th>wavefunction</th>\n",
              "      <th>wavefunction-collapse</th>\n",
              "      <th>waveguide</th>\n",
              "      <th>wavelength</th>\n",
              "      <th>waves</th>\n",
              "      <th>weak-interaction</th>\n",
              "      <th>weather</th>\n",
              "      <th>weight</th>\n",
              "      <th>white-dwarfs</th>\n",
              "      <th>white-holes</th>\n",
              "      <th>wick-rotation</th>\n",
              "      <th>wick-theorem</th>\n",
              "      <th>wightman-fields</th>\n",
              "      <th>wigner-eckart</th>\n",
              "      <th>wigner-transform</th>\n",
              "      <th>wilson-loop</th>\n",
              "      <th>wimps</th>\n",
              "      <th>work</th>\n",
              "      <th>wormholes</th>\n",
              "      <th>x-ray-crystallography</th>\n",
              "      <th>x-rays</th>\n",
              "      <th>yang-mills</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i often hear about subatomic particles having ...</td>\n",
              "      <td>quantum-mechanics  particle-physics  angular-...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how would you explain string theory to non phy...</td>\n",
              "      <td>string-theory  education</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is a question that has been posted at man...</td>\n",
              "      <td>particle-physics  group-theory  representatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the main problems that we need to sol...</td>\n",
              "      <td>quantum-mechanics  quantum-interpretations  h...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hamilton s principle states that a dynamic sys...</td>\n",
              "      <td>lagrangian-formalism  variational-principle  ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 725 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Body  ... yang-mills\n",
              "0  i often hear about subatomic particles having ...  ...          0\n",
              "1  how would you explain string theory to non phy...  ...          0\n",
              "2  this is a question that has been posted at man...  ...          0\n",
              "3  what are the main problems that we need to sol...  ...          0\n",
              "4  hamilton s principle states that a dynamic sys...  ...          0\n",
              "\n",
              "[5 rows x 725 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:26.152565Z",
          "iopub.execute_input": "2021-07-21T16:17:26.152937Z",
          "iopub.status.idle": "2021-07-21T16:17:26.264583Z",
          "shell.execute_reply.started": "2021-07-21T16:17:26.152900Z",
          "shell.execute_reply": "2021-07-21T16:17:26.263642Z"
        },
        "trusted": true,
        "id": "Yy43Zkef4dhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3188b75-25f6-4ee6-c9f9-950f359c1f07"
      },
      "source": [
        "#list of all tags\n",
        "def clean_tag(line):\n",
        "    line = re.sub('[<>]', ' ', line)\n",
        "    return line\n",
        "df.Tags = df.Tags.apply(lambda x: clean_tag(x))\n",
        "vectorizer = CountVectorizer(tokenizer= lambda text : text.split(\" \"))\n",
        "tag_dtm = vectorizer.fit_transform(df[\"Tags\"])\n",
        "tags = vectorizer.get_feature_names()\n",
        "tags = [x for x in tags if x]\n",
        "len_rows = df.shape[0]\n",
        "len_tags = len(tags)\n",
        "print('Total no. of unique tags: ',len_tags)\n",
        "print('dataframe size: ', len_rows)\n",
        "tags_count = {}\n",
        "temp_series = df_tags.sum().sort_values(ascending=False)\n",
        "for i in range(len_tags):\n",
        "    tags_count[temp_series.index[i]] = temp_series.values[i]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no. of unique tags:  723\n",
            "dataframe size:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:17:32.240195Z",
          "iopub.execute_input": "2021-07-21T16:17:32.240518Z",
          "iopub.status.idle": "2021-07-21T16:20:37.196504Z",
          "shell.execute_reply.started": "2021-07-21T16:17:32.240489Z",
          "shell.execute_reply": "2021-07-21T16:20:37.195276Z"
        },
        "trusted": true,
        "id": "p1qRcHDi4dhC"
      },
      "source": [
        "# !wget 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "# !unzip 'glove.6B.zip'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:07.447504Z",
          "iopub.execute_input": "2021-07-21T16:21:07.447905Z",
          "iopub.status.idle": "2021-07-21T16:21:39.318540Z",
          "shell.execute_reply.started": "2021-07-21T16:21:07.447866Z",
          "shell.execute_reply": "2021-07-21T16:21:39.317674Z"
        },
        "trusted": true,
        "id": "QztKEl8D4dhC"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:42.510215Z",
          "iopub.execute_input": "2021-07-21T16:21:42.510664Z",
          "iopub.status.idle": "2021-07-21T16:21:42.519939Z",
          "shell.execute_reply.started": "2021-07-21T16:21:42.510609Z",
          "shell.execute_reply": "2021-07-21T16:21:42.519067Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV2VsUIf4dhD",
        "outputId": "6c5e0216-f938-4c14-c9ba-3950d54b5279"
      },
      "source": [
        "tags[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1pi-effective-action',\n",
              " 'absolute-units',\n",
              " 'absorption',\n",
              " 'acceleration',\n",
              " 'accelerator-physics',\n",
              " 'accretion-disk',\n",
              " 'acoustics',\n",
              " 'action',\n",
              " 'adiabatic',\n",
              " 'adm-formalism']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:43.889095Z",
          "iopub.execute_input": "2021-07-21T16:21:43.889438Z",
          "iopub.status.idle": "2021-07-21T16:21:43.908892Z",
          "shell.execute_reply.started": "2021-07-21T16:21:43.889408Z",
          "shell.execute_reply": "2021-07-21T16:21:43.907992Z"
        },
        "trusted": true,
        "id": "YJSkIGj04dhD"
      },
      "source": [
        "#getting tag embeddings\n",
        "tag_embeddings = []\n",
        "count = 0\n",
        "for tag in tags:\n",
        "    sum = np.zeros(100)\n",
        "    try:\n",
        "        sub_tags = tag.split('-')\n",
        "        for x in sub_tags:\n",
        "            try:\n",
        "                sum += embeddings_index[x]\n",
        "            except:\n",
        "                continue\n",
        "        tag_embeddings.append(sum/len(sub_tags))\n",
        "    except:\n",
        "        count+=1\n",
        "        print(tag)\n",
        "tag_embeddings = np.array(tag_embeddings)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:44.581009Z",
          "iopub.execute_input": "2021-07-21T16:21:44.581327Z",
          "iopub.status.idle": "2021-07-21T16:21:44.603784Z",
          "shell.execute_reply.started": "2021-07-21T16:21:44.581293Z",
          "shell.execute_reply": "2021-07-21T16:21:44.602838Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "XFyWfW4v4dhE",
        "outputId": "0205d921-afc9-4ffb-eac8-0cd2a9ec3719"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "      <th>1pi-effective-action</th>\n",
              "      <th>absolute-units</th>\n",
              "      <th>absorption</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>accelerator-physics</th>\n",
              "      <th>accretion-disk</th>\n",
              "      <th>acoustics</th>\n",
              "      <th>action</th>\n",
              "      <th>adiabatic</th>\n",
              "      <th>adm-formalism</th>\n",
              "      <th>ads-cft</th>\n",
              "      <th>aerodynamics</th>\n",
              "      <th>aether</th>\n",
              "      <th>affine-lie-algebra</th>\n",
              "      <th>air</th>\n",
              "      <th>aircraft</th>\n",
              "      <th>algebraic-geometry</th>\n",
              "      <th>algebraic-topology</th>\n",
              "      <th>algorithms</th>\n",
              "      <th>amorphous-solids</th>\n",
              "      <th>analyticity</th>\n",
              "      <th>angular-momentum</th>\n",
              "      <th>angular-velocity</th>\n",
              "      <th>antennas</th>\n",
              "      <th>anthropic-principle</th>\n",
              "      <th>anti-de-sitter-spacetime</th>\n",
              "      <th>anticommutator</th>\n",
              "      <th>antimatter</th>\n",
              "      <th>anyons</th>\n",
              "      <th>applied-physics</th>\n",
              "      <th>approximations</th>\n",
              "      <th>arrow-of-time</th>\n",
              "      <th>asteroids</th>\n",
              "      <th>astrometrics</th>\n",
              "      <th>astronomy</th>\n",
              "      <th>astrophotography</th>\n",
              "      <th>astrophysics</th>\n",
              "      <th>asymptotics</th>\n",
              "      <th>...</th>\n",
              "      <th>variational-calculus</th>\n",
              "      <th>variational-principle</th>\n",
              "      <th>vector-fields</th>\n",
              "      <th>vectors</th>\n",
              "      <th>velocity</th>\n",
              "      <th>vibrations</th>\n",
              "      <th>virial-theorem</th>\n",
              "      <th>virtual-particles</th>\n",
              "      <th>viscosity</th>\n",
              "      <th>visible-light</th>\n",
              "      <th>vision</th>\n",
              "      <th>visualization</th>\n",
              "      <th>voltage</th>\n",
              "      <th>volume</th>\n",
              "      <th>vortex</th>\n",
              "      <th>warp-drives</th>\n",
              "      <th>water</th>\n",
              "      <th>wave-particle-duality</th>\n",
              "      <th>wavefunction</th>\n",
              "      <th>wavefunction-collapse</th>\n",
              "      <th>waveguide</th>\n",
              "      <th>wavelength</th>\n",
              "      <th>waves</th>\n",
              "      <th>weak-interaction</th>\n",
              "      <th>weather</th>\n",
              "      <th>weight</th>\n",
              "      <th>white-dwarfs</th>\n",
              "      <th>white-holes</th>\n",
              "      <th>wick-rotation</th>\n",
              "      <th>wick-theorem</th>\n",
              "      <th>wightman-fields</th>\n",
              "      <th>wigner-eckart</th>\n",
              "      <th>wigner-transform</th>\n",
              "      <th>wilson-loop</th>\n",
              "      <th>wimps</th>\n",
              "      <th>work</th>\n",
              "      <th>wormholes</th>\n",
              "      <th>x-ray-crystallography</th>\n",
              "      <th>x-rays</th>\n",
              "      <th>yang-mills</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i often hear about subatomic particles having ...</td>\n",
              "      <td>quantum-mechanics  particle-physics  angular-...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how would you explain string theory to non phy...</td>\n",
              "      <td>string-theory  education</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is a question that has been posted at man...</td>\n",
              "      <td>particle-physics  group-theory  representatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the main problems that we need to sol...</td>\n",
              "      <td>quantum-mechanics  quantum-interpretations  h...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hamilton s principle states that a dynamic sys...</td>\n",
              "      <td>lagrangian-formalism  variational-principle  ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 725 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Body  ... yang-mills\n",
              "0  i often hear about subatomic particles having ...  ...          0\n",
              "1  how would you explain string theory to non phy...  ...          0\n",
              "2  this is a question that has been posted at man...  ...          0\n",
              "3  what are the main problems that we need to sol...  ...          0\n",
              "4  hamilton s principle states that a dynamic sys...  ...          0\n",
              "\n",
              "[5 rows x 725 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:46.437254Z",
          "iopub.execute_input": "2021-07-21T16:21:46.437579Z",
          "iopub.status.idle": "2021-07-21T16:21:50.511355Z",
          "shell.execute_reply.started": "2021-07-21T16:21:46.437547Z",
          "shell.execute_reply": "2021-07-21T16:21:50.510464Z"
        },
        "trusted": true,
        "id": "ncghruIJ4dhE"
      },
      "source": [
        "#importing libraries\n",
        "import tensorflow as tf\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.activations import sigmoid\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Embedding, GlobalMaxPooling1D, dot, Reshape, Layer\n",
        "from tensorflow.keras.backend import variable, dot as k_dot, sigmoid, relu\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from tensorflow import Variable\n",
        "# from spektral.layers import GCNConv\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LxbzNwA4dhE"
      },
      "source": [
        "### Embedding of questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:50.512991Z",
          "iopub.execute_input": "2021-07-21T16:21:50.513387Z",
          "iopub.status.idle": "2021-07-21T16:21:50.521974Z",
          "shell.execute_reply.started": "2021-07-21T16:21:50.513350Z",
          "shell.execute_reply": "2021-07-21T16:21:50.521091Z"
        },
        "trusted": true,
        "id": "7yRZnpqv4dhF"
      },
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])\n",
        "\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:50.523828Z",
          "iopub.execute_input": "2021-07-21T16:21:50.524198Z",
          "iopub.status.idle": "2021-07-21T16:21:50.551282Z",
          "shell.execute_reply.started": "2021-07-21T16:21:50.524162Z",
          "shell.execute_reply": "2021-07-21T16:21:50.550082Z"
        },
        "trusted": true,
        "id": "TGYvfdij4dhF"
      },
      "source": [
        "X = df['Body']\n",
        "y = df.drop(['Body', 'Tags'], axis=1).values"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:50.553422Z",
          "iopub.execute_input": "2021-07-21T16:21:50.554001Z",
          "iopub.status.idle": "2021-07-21T16:21:51.530691Z",
          "shell.execute_reply.started": "2021-07-21T16:21:50.553957Z",
          "shell.execute_reply": "2021-07-21T16:21:51.529672Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nCVrH6X4dhF",
        "outputId": "5531648d-64a6-43d5-bf36-082c4bceb7a2"
      },
      "source": [
        "tokenizer = create_tokenizer(X)\n",
        "length = max_length(X)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "X = encode_text(tokenizer, X, length)\n",
        "print(X.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 3325\n",
            "Vocabulary size: 24280\n",
            "(10000, 3325)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT7iwqXn1Ezt"
      },
      "source": [
        "def get_emb_matrix(embeddings_index, vocab_size, emb_size = 100):\n",
        "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "    vocab_to_idx = {}\n",
        "    vocab = []\n",
        "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "    i = 0\n",
        "    for word in tokenizer.word_index:\n",
        "        if word in embeddings_index:\n",
        "            W[i] = embeddings_index[word]\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "        vocab_to_idx[word] = i\n",
        "        vocab.append(word)\n",
        "        i += 1   \n",
        "    return W, np.array(vocab), vocab_to_idx\n",
        "pretrained_weights, vocab, vocab2index = get_emb_matrix(embeddings_index, vocab_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:52.511827Z",
          "iopub.execute_input": "2021-07-21T16:21:52.512131Z",
          "iopub.status.idle": "2021-07-21T16:21:52.635268Z",
          "shell.execute_reply.started": "2021-07-21T16:21:52.512099Z",
          "shell.execute_reply": "2021-07-21T16:21:52.634189Z"
        },
        "trusted": true,
        "id": "RM6B1l_J4dhG"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CvLqk94dhG"
      },
      "source": [
        "### Adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:52.636752Z",
          "iopub.execute_input": "2021-07-21T16:21:52.637191Z",
          "iopub.status.idle": "2021-07-21T16:21:53.414850Z",
          "shell.execute_reply.started": "2021-07-21T16:21:52.637141Z",
          "shell.execute_reply": "2021-07-21T16:21:53.413942Z"
        },
        "trusted": true,
        "id": "nClnjH_U4dhG"
      },
      "source": [
        "adj_matrix_path = 'adjacency_matrix.json'\n",
        "# Count all labels.\n",
        "nums = np.sum(np.array(df_tags), axis=0)\n",
        "adj = np.zeros((len_tags, len_tags), dtype=int)\n",
        "# Now iterate over the whole training set and consider all pairs of labels in sample annotation.\n",
        "for sample in np.array(df_tags):\n",
        "    sample_idx = np.argwhere(sample > 0)[:, 0]\n",
        "    # We count all possible pairs that can be created from each sample's set of labels.\n",
        "    for i, j in itertools.combinations(sample_idx, 2):\n",
        "        adj[i, j] += 1\n",
        "        adj[j, i] += 1\n",
        "# Save it for further use.        \n",
        "with open(adj_matrix_path, 'w') as fp:\n",
        "    json.dump({\n",
        "        'nums': nums.tolist(),\n",
        "        'adj': adj.tolist()\n",
        "    }, fp, indent=3)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:53.416895Z",
          "iopub.execute_input": "2021-07-21T16:21:53.417242Z",
          "iopub.status.idle": "2021-07-21T16:21:53.423022Z",
          "shell.execute_reply.started": "2021-07-21T16:21:53.417205Z",
          "shell.execute_reply": "2021-07-21T16:21:53.422106Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvKWt4qn4dhH",
        "outputId": "3c8a3265-3adb-4a64-a2b6-0d146ace49e2"
      },
      "source": [
        "# GCNConv.preprocess(adj).astype('f4')\n",
        "tag_embeddings.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(723, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBLmAmLF4dhH"
      },
      "source": [
        "### Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:58.061490Z",
          "iopub.execute_input": "2021-07-21T16:21:58.061967Z",
          "iopub.status.idle": "2021-07-21T16:21:59.965096Z",
          "shell.execute_reply.started": "2021-07-21T16:21:58.061924Z",
          "shell.execute_reply": "2021-07-21T16:21:59.964278Z"
        },
        "trusted": true,
        "id": "v3pXvIb04dhH"
      },
      "source": [
        "# def custom_model(length, vocab_size, tag_embeddings, adj):\n",
        "#     inputs1 = Input(shape=(length, ))\n",
        "#     embedding = Embedding(vocab_size, 100)(inputs1)\n",
        "#     conv = Conv1D(filters=100, kernel_size=4, activation='relu')(embedding)\n",
        "#     pool = GlobalMaxPooling1D()(conv)\n",
        "#     model = Model(inputs1, pool)\n",
        "#     print(model.summary())\n",
        "    \n",
        "#     X_in = Input(shape=(len(tag_embeddings[0]), ))\n",
        "# #     fltr_in = Input((len(tag_embeddings), ))\n",
        "      \n",
        "#     graph_conv_1 = GCNConv(50, activation='relu')([X_in, adj])\n",
        "#     graph_conv_2 = GCNConv(50, activation='relu')([graph_conv_1, adj])\n",
        "#     print(pool.shape)\n",
        "#     print(graph_conv_2.shape)\n",
        "#     output = tf.matmul(pool, tf.transpose(graph_conv_2))\n",
        "#     output = sigmoid(output)\n",
        "#     print(output.shape)\n",
        "    \n",
        "#     gcn = Model(inputs = [inputs1, X_in], outputs = output)\n",
        "#     print(gcn.summary())\n",
        "#     gcn.compile(optimizer='adam', loss='binary_crossentropy', weighted_metrics=['acc'])\n",
        "#     gcn.fit([X, tag_embeddings] , y.values, epochs=10)\n",
        "# custom_model(length, vocab_size, tag_embeddings, adj)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:21:59.966590Z",
          "iopub.execute_input": "2021-07-21T16:21:59.966953Z",
          "iopub.status.idle": "2021-07-21T16:21:59.972067Z",
          "shell.execute_reply.started": "2021-07-21T16:21:59.966917Z",
          "shell.execute_reply": "2021-07-21T16:21:59.971232Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdJGBss34dhI",
        "outputId": "34d3c4cb-fa24-4528-b3df-556f7c169393"
      },
      "source": [
        "print(length)\n",
        "print(vocab_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3325\n",
            "24280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:00.201060Z",
          "iopub.execute_input": "2021-07-21T16:22:00.201322Z",
          "iopub.status.idle": "2021-07-21T16:22:00.209162Z",
          "shell.execute_reply.started": "2021-07-21T16:22:00.201297Z",
          "shell.execute_reply": "2021-07-21T16:22:00.208176Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dopOkjuK4dhJ",
        "outputId": "48037645-3d53-4e31-8d07-0ec2f1340264"
      },
      "source": [
        "tag_embeddings.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(723, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrbFw3DR4dhJ"
      },
      "source": [
        "### Pytorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:04.145620Z",
          "iopub.execute_input": "2021-07-21T16:22:04.146006Z",
          "iopub.status.idle": "2021-07-21T16:22:05.591702Z",
          "shell.execute_reply.started": "2021-07-21T16:22:04.145972Z",
          "shell.execute_reply": "2021-07-21T16:22:05.590844Z"
        },
        "trusted": true,
        "id": "CY_gFx0I4dhJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device('cuda')\n",
        "tag_embeddings = torch.from_numpy(tag_embeddings)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:07.210913Z",
          "iopub.execute_input": "2021-07-21T16:22:07.211299Z",
          "iopub.status.idle": "2021-07-21T16:22:07.219447Z",
          "shell.execute_reply.started": "2021-07-21T16:22:07.211263Z",
          "shell.execute_reply": "2021-07-21T16:22:07.218466Z"
        },
        "trusted": true,
        "id": "16TmwscS4dhJ"
      },
      "source": [
        "#dataloader class for batch training\n",
        "class QuestionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        #data loading\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "        #self.tag_embeddings = torch.from_numpy(tag_embeddings)\n",
        "        self.n_samples = X.shape[0]\n",
        "    def __getitem__(self, item):\n",
        "        #dataset[0]\n",
        "        return self.X[item], self.y[item]\n",
        "    def __len__(self):\n",
        "        #le(dataset)\n",
        "        return self.n_samples\n",
        "# data = QuestionDataset()\n",
        "# a, b, c = data[0]\n",
        "# print(a.shape, b.shape, c.shape)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:08.152009Z",
          "iopub.execute_input": "2021-07-21T16:22:08.152336Z",
          "iopub.status.idle": "2021-07-21T16:22:08.165430Z",
          "shell.execute_reply.started": "2021-07-21T16:22:08.152304Z",
          "shell.execute_reply": "2021-07-21T16:22:08.164286Z"
        },
        "trusted": true,
        "id": "ASdja_FA4dhK"
      },
      "source": [
        "# function for creating adjacency matrix (over-smoothing problem is also taken into consideration)\n",
        "'''\n",
        "num_classes ---> no. of unique tags\n",
        "t ---> threshold for binarizing the correlation matrix.\n",
        "p ---> determines the weights assigned to a node itself and other correlated nodes\n",
        "adj_data ---> contains co-occurrence matrix and corresponding tag counts\n",
        "    \n",
        "*Note -> for info. regarding all these parameters go to https://arxiv.org/pdf/1904.03582.pdf\n",
        "'''\n",
        "\n",
        "t = 0.4\n",
        "p = 0.2\n",
        "def gen_A(len_tags, t, p, adj_data):\n",
        "    adj = np.array(adj_data['adj']).astype(np.float32)\n",
        "    nums = np.array(adj_data['nums']).astype(np.float32)\n",
        "    nums = nums[:, np.newaxis]\n",
        "    adj = adj / nums\n",
        "    adj[adj < t] = 0\n",
        "    adj[adj >= t] = 1\n",
        "    adj = adj * p / (adj.sum(0, keepdims=True) + 1e-6)  \n",
        "    adj = adj + np.identity(len_tags, np.int)\n",
        "    return adj\n",
        "\n",
        "#normalization of Adjacency matrix\n",
        "def gen_adj(A):\n",
        "    D = torch.pow(A.sum(1).float(), -0.5)\n",
        "    D = torch.diag(D).type_as(A)\n",
        "    adj = torch.matmul(torch.matmul(A, D).t(), D)\n",
        "    return adj\n",
        "\n",
        "# Use threshold to define predicted labels and invoke sklearn's metrics with different averaging strategies.\n",
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "    pred = np.array(pred > threshold, dtype=int)\n",
        "    return {\n",
        "            'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
        "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
        "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
        "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
        "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
        "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
        "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
        "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
        "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
        "            }"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:08.931829Z",
          "iopub.execute_input": "2021-07-21T16:22:08.932154Z",
          "iopub.status.idle": "2021-07-21T16:22:13.123265Z",
          "shell.execute_reply.started": "2021-07-21T16:22:08.932125Z",
          "shell.execute_reply": "2021-07-21T16:22:13.121810Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2WYxqdz4dhK",
        "outputId": "2d8c9936-f585-4bc8-8bbc-39afacab2ec8"
      },
      "source": [
        "#defining Graph Convolution Network\n",
        "\"\"\"\n",
        "Simple GCN layer (pytorch), similar to https://arxiv.org/abs/1609.02907\n",
        "\n",
        "\"\"\"\n",
        "class GraphConvolution(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_features, out_features, bias=False):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features), requires_grad=True)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(1, 1, out_features), requires_grad=True)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.matmul(input.float(), self.weight)\n",
        "        output = torch.matmul(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class TagRetrieval(nn.Module):\n",
        "    \n",
        "    def __init__(self, len_tags, adj_matrix_path, vocab_size, embedding_dim, glove_weights, t=0.1, p=0.25):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "        self.conv1d = nn.Conv1d(embedding_dim, embedding_dim, 4, padding=0)\n",
        "        #self.globalmaxpool1d = nn.AdaptiveMaxPool2d(output_size=(embedding_dim, 1))\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.linear = nn.Linear(length*embedding_dim, 50)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.gc1 = GraphConvolution(embedding_dim, embedding_dim)\n",
        "        self.gc2 = GraphConvolution(embedding_dim, embedding_dim)\n",
        "        self.relu = nn.LeakyReLU(0.2)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        # Load data for adjacency matrix\n",
        "        with open(adj_matrix_path) as fp:\n",
        "            adj_data = json.load(fp)\n",
        "        # Compute adjacency matrix\n",
        "        adj = gen_A(len_tags, t, p, adj_data)\n",
        "        self.A = nn.Parameter(torch.from_numpy(adj).float(), requires_grad=False)\n",
        "        \n",
        "    def forward(self, question, tags_embeddings):\n",
        "        # Get visual features from image\n",
        "        #feature = self.features(imgs)\n",
        "        #feature = feature.view(feature.size(0), -1)\n",
        "        embedding = self.embedding(question)\n",
        "        embedding = embedding.permute([0, 2, 1])\n",
        "#         embedding = self.dropout(embedding)\n",
        "        conv1d = self.dropout(embedding)\n",
        "        conv1d = self.conv1d(embedding)\n",
        "#         globalmaxpool1d = self.globalmaxpool1d(conv1d)\n",
        "#         feature = globalmaxpool1d.squeeze()\n",
        "        conv1d = conv1d.permute([0, 2, 1])\n",
        "        feature, _  = torch.max(conv1d, 1)\n",
        "#         feature = self.linear(self.flatten(embedding))\n",
        "        # Get graph features from graph\n",
        "        #inp = inp[0].squeeze()\n",
        "        inp = tag_embeddings\n",
        "        adj = gen_adj(self.A).detach()\n",
        "        x = self.gc1(inp, adj)\n",
        "        x = self.relu(x)\n",
        "        x = self.gc2(x, adj)\n",
        "        \n",
        "        # We multiply the features from GÐ¡N and Ð¡NN in order to take into account \n",
        "        # the contribution to the prediction of classes from both the text and the graph.\n",
        "        x = x.transpose(0, 1)\n",
        "        x = torch.matmul(feature, x)\n",
        "        return self.sigm(x)\n",
        "    \n",
        "model = TagRetrieval(len_tags, adj_matrix_path, vocab_size, 100, pretrained_weights)\n",
        "model = model.to(device)\n",
        "print(model.parameters)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.parameters of TagRetrieval(\n",
            "  (embedding): Embedding(24280, 100, padding_idx=0)\n",
            "  (conv1d): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (gc1): GraphConvolution (100 -> 100)\n",
            "  (gc2): GraphConvolution (100 -> 100)\n",
            "  (relu): LeakyReLU(negative_slope=0.2)\n",
            "  (sigm): Sigmoid()\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:13.124840Z",
          "iopub.execute_input": "2021-07-21T16:22:13.125194Z",
          "iopub.status.idle": "2021-07-21T16:22:13.131732Z",
          "shell.execute_reply.started": "2021-07-21T16:22:13.125147Z",
          "shell.execute_reply": "2021-07-21T16:22:13.130825Z"
        },
        "trusted": true,
        "id": "jpzsXRVZ4dhM"
      },
      "source": [
        "batch_size = 32\n",
        "num_workers = 8\n",
        "lr = 0.001\n",
        "max_epoch_number = 100"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:13.133313Z",
          "iopub.execute_input": "2021-07-21T16:22:13.135162Z",
          "iopub.status.idle": "2021-07-21T16:22:13.141393Z",
          "shell.execute_reply.started": "2021-07-21T16:22:13.135122Z",
          "shell.execute_reply": "2021-07-21T16:22:13.140602Z"
        },
        "trusted": true,
        "id": "5IKutJgW4dhM"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:13.142907Z",
          "iopub.execute_input": "2021-07-21T16:22:13.143390Z",
          "iopub.status.idle": "2021-07-21T16:22:13.150042Z",
          "shell.execute_reply.started": "2021-07-21T16:22:13.143351Z",
          "shell.execute_reply": "2021-07-21T16:22:13.149095Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgX2rbT24dhM",
        "outputId": "c53a2e3c-c612-4d02-a823-552c37f95134"
      },
      "source": [
        "train_dataset = QuestionDataset(X_train, y_train)\n",
        "val_dataset = QuestionDataset(X_val, y_val)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "num_train_batches = int(np.ceil(train_dataset.__len__()/ batch_size))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T16:22:16.297686Z",
          "iopub.execute_input": "2021-07-21T16:22:16.298019Z",
          "iopub.status.idle": "2021-07-21T16:22:17.915783Z",
          "shell.execute_reply.started": "2021-07-21T16:22:16.297989Z",
          "shell.execute_reply": "2021-07-21T16:22:17.914180Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wy21N524dhN",
        "outputId": "ee3e8006-dc99-4a7b-d4b7-571a5dd41f06"
      },
      "source": [
        "for epoch in range(max_epoch_number):\n",
        "    batch_losses = []\n",
        "    for batch_number, (X, y) in enumerate(train_dataloader):\n",
        "        X, y, tag_embeddings = X.to(device), y.to(device), tag_embeddings.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model_result = model(X.long(), tag_embeddings)\n",
        "        loss = criterion(model_result, y.float())\n",
        "        batch_loss_value = loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_losses.append(batch_loss_value)\n",
        "        if (batch_number+1)%50==0:\n",
        "            print(f'epoch: {epoch+1}/{max_epoch_number}, step: {batch_number+1}/{num_train_batches}, loss: {batch_loss_value}')\n",
        "    loss_value = np.mean(batch_losses)\n",
        "    print(\"epoch:{:2d} loss:{:.3f}\".format(epoch, loss_value))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/100, step: 50/250, loss: 0.04172871261835098\n",
            "epoch: 1/100, step: 100/250, loss: 0.03510912507772446\n",
            "epoch: 1/100, step: 150/250, loss: 0.031027497723698616\n",
            "epoch: 1/100, step: 200/250, loss: 0.034056104719638824\n",
            "epoch: 1/100, step: 250/250, loss: 0.031450215727090836\n",
            "epoch: 0 loss:0.043\n",
            "epoch: 2/100, step: 50/250, loss: 0.024928493425250053\n",
            "epoch: 2/100, step: 100/250, loss: 0.02843504771590233\n",
            "epoch: 2/100, step: 150/250, loss: 0.026775911450386047\n",
            "epoch: 2/100, step: 200/250, loss: 0.027312731370329857\n",
            "epoch: 2/100, step: 250/250, loss: 0.027582045644521713\n",
            "epoch: 1 loss:0.027\n",
            "epoch: 3/100, step: 50/250, loss: 0.024381250143051147\n",
            "epoch: 3/100, step: 100/250, loss: 0.025468576699495316\n",
            "epoch: 3/100, step: 150/250, loss: 0.023530077189207077\n",
            "epoch: 3/100, step: 200/250, loss: 0.027302557602524757\n",
            "epoch: 3/100, step: 250/250, loss: 0.024640686810016632\n",
            "epoch: 2 loss:0.025\n",
            "epoch: 4/100, step: 50/250, loss: 0.022720878943800926\n",
            "epoch: 4/100, step: 100/250, loss: 0.02478722296655178\n",
            "epoch: 4/100, step: 150/250, loss: 0.02635176107287407\n",
            "epoch: 4/100, step: 200/250, loss: 0.023534689098596573\n",
            "epoch: 4/100, step: 250/250, loss: 0.025501256808638573\n",
            "epoch: 3 loss:0.024\n",
            "epoch: 5/100, step: 50/250, loss: 0.022277913987636566\n",
            "epoch: 5/100, step: 100/250, loss: 0.022879235446453094\n",
            "epoch: 5/100, step: 150/250, loss: 0.024233663454651833\n",
            "epoch: 5/100, step: 200/250, loss: 0.022819355130195618\n",
            "epoch: 5/100, step: 250/250, loss: 0.025344960391521454\n",
            "epoch: 4 loss:0.024\n",
            "epoch: 6/100, step: 50/250, loss: 0.023496106266975403\n",
            "epoch: 6/100, step: 100/250, loss: 0.021904561668634415\n",
            "epoch: 6/100, step: 150/250, loss: 0.02205708622932434\n",
            "epoch: 6/100, step: 200/250, loss: 0.024662360548973083\n",
            "epoch: 6/100, step: 250/250, loss: 0.0215629693120718\n",
            "epoch: 5 loss:0.023\n",
            "epoch: 7/100, step: 50/250, loss: 0.020794352516531944\n",
            "epoch: 7/100, step: 100/250, loss: 0.021323276683688164\n",
            "epoch: 7/100, step: 150/250, loss: 0.022546418011188507\n",
            "epoch: 7/100, step: 200/250, loss: 0.019743379205465317\n",
            "epoch: 7/100, step: 250/250, loss: 0.02627679891884327\n",
            "epoch: 6 loss:0.022\n",
            "epoch: 8/100, step: 50/250, loss: 0.018829358741641045\n",
            "epoch: 8/100, step: 100/250, loss: 0.02475673332810402\n",
            "epoch: 8/100, step: 150/250, loss: 0.019853699952363968\n",
            "epoch: 8/100, step: 200/250, loss: 0.018512465059757233\n",
            "epoch: 8/100, step: 250/250, loss: 0.020011311396956444\n",
            "epoch: 7 loss:0.021\n",
            "epoch: 9/100, step: 50/250, loss: 0.01792728714644909\n",
            "epoch: 9/100, step: 100/250, loss: 0.018484191969037056\n",
            "epoch: 9/100, step: 150/250, loss: 0.017457056790590286\n",
            "epoch: 9/100, step: 200/250, loss: 0.017809325829148293\n",
            "epoch: 9/100, step: 250/250, loss: 0.017454132437705994\n",
            "epoch: 8 loss:0.019\n",
            "epoch: 10/100, step: 50/250, loss: 0.015759050846099854\n",
            "epoch: 10/100, step: 100/250, loss: 0.02035113424062729\n",
            "epoch: 10/100, step: 150/250, loss: 0.0194131787866354\n",
            "epoch: 10/100, step: 200/250, loss: 0.01781090721487999\n",
            "epoch: 10/100, step: 250/250, loss: 0.01856466569006443\n",
            "epoch: 9 loss:0.018\n",
            "epoch: 11/100, step: 50/250, loss: 0.01868865266442299\n",
            "epoch: 11/100, step: 100/250, loss: 0.017768705263733864\n",
            "epoch: 11/100, step: 150/250, loss: 0.016395313665270805\n",
            "epoch: 11/100, step: 200/250, loss: 0.019342737272381783\n",
            "epoch: 11/100, step: 250/250, loss: 0.02017807587981224\n",
            "epoch:10 loss:0.017\n",
            "epoch: 12/100, step: 50/250, loss: 0.01697169616818428\n",
            "epoch: 12/100, step: 100/250, loss: 0.016558421775698662\n",
            "epoch: 12/100, step: 150/250, loss: 0.017306014895439148\n",
            "epoch: 12/100, step: 200/250, loss: 0.01641054078936577\n",
            "epoch: 12/100, step: 250/250, loss: 0.01591622829437256\n",
            "epoch:11 loss:0.016\n",
            "epoch: 13/100, step: 50/250, loss: 0.016346357762813568\n",
            "epoch: 13/100, step: 100/250, loss: 0.014749805442988873\n",
            "epoch: 13/100, step: 150/250, loss: 0.01459592767059803\n",
            "epoch: 13/100, step: 200/250, loss: 0.016607675701379776\n",
            "epoch: 13/100, step: 250/250, loss: 0.015272868797183037\n",
            "epoch:12 loss:0.016\n",
            "epoch: 14/100, step: 50/250, loss: 0.014764046296477318\n",
            "epoch: 14/100, step: 100/250, loss: 0.015344060957431793\n",
            "epoch: 14/100, step: 150/250, loss: 0.014016290195286274\n",
            "epoch: 14/100, step: 200/250, loss: 0.013382251374423504\n",
            "epoch: 14/100, step: 250/250, loss: 0.013598203659057617\n",
            "epoch:13 loss:0.015\n",
            "epoch: 15/100, step: 50/250, loss: 0.015256241895258427\n",
            "epoch: 15/100, step: 100/250, loss: 0.014636767096817493\n",
            "epoch: 15/100, step: 150/250, loss: 0.014214280061423779\n",
            "epoch: 15/100, step: 200/250, loss: 0.012087349779903889\n",
            "epoch: 15/100, step: 250/250, loss: 0.014112871140241623\n",
            "epoch:14 loss:0.014\n",
            "epoch: 16/100, step: 50/250, loss: 0.012950877659022808\n",
            "epoch: 16/100, step: 100/250, loss: 0.013700941577553749\n",
            "epoch: 16/100, step: 150/250, loss: 0.013996697030961514\n",
            "epoch: 16/100, step: 200/250, loss: 0.014175597578287125\n",
            "epoch: 16/100, step: 250/250, loss: 0.013195766136050224\n",
            "epoch:15 loss:0.013\n",
            "epoch: 17/100, step: 50/250, loss: 0.012882892973721027\n",
            "epoch: 17/100, step: 100/250, loss: 0.010944075882434845\n",
            "epoch: 17/100, step: 150/250, loss: 0.014283062890172005\n",
            "epoch: 17/100, step: 200/250, loss: 0.01306962501257658\n",
            "epoch: 17/100, step: 250/250, loss: 0.01261583436280489\n",
            "epoch:16 loss:0.012\n",
            "epoch: 18/100, step: 50/250, loss: 0.012080336920917034\n",
            "epoch: 18/100, step: 100/250, loss: 0.013369820080697536\n",
            "epoch: 18/100, step: 150/250, loss: 0.012019169516861439\n",
            "epoch: 18/100, step: 200/250, loss: 0.011251607909798622\n",
            "epoch: 18/100, step: 250/250, loss: 0.010339861735701561\n",
            "epoch:17 loss:0.012\n",
            "epoch: 19/100, step: 50/250, loss: 0.010127292014658451\n",
            "epoch: 19/100, step: 100/250, loss: 0.011431092396378517\n",
            "epoch: 19/100, step: 150/250, loss: 0.011720763519406319\n",
            "epoch: 19/100, step: 200/250, loss: 0.009953158907592297\n",
            "epoch: 19/100, step: 250/250, loss: 0.013990798033773899\n",
            "epoch:18 loss:0.011\n",
            "epoch: 20/100, step: 50/250, loss: 0.010918524116277695\n",
            "epoch: 20/100, step: 100/250, loss: 0.009430455975234509\n",
            "epoch: 20/100, step: 150/250, loss: 0.010405279695987701\n",
            "epoch: 20/100, step: 200/250, loss: 0.010424979962408543\n",
            "epoch: 20/100, step: 250/250, loss: 0.010887597687542439\n",
            "epoch:19 loss:0.011\n",
            "epoch: 21/100, step: 50/250, loss: 0.008657075464725494\n",
            "epoch: 21/100, step: 100/250, loss: 0.009991677477955818\n",
            "epoch: 21/100, step: 150/250, loss: 0.010192903690040112\n",
            "epoch: 21/100, step: 200/250, loss: 0.011082811281085014\n",
            "epoch: 21/100, step: 250/250, loss: 0.010622501373291016\n",
            "epoch:20 loss:0.010\n",
            "epoch: 22/100, step: 50/250, loss: 0.01070654857903719\n",
            "epoch: 22/100, step: 100/250, loss: 0.01101314090192318\n",
            "epoch: 22/100, step: 150/250, loss: 0.008838918060064316\n",
            "epoch: 22/100, step: 200/250, loss: 0.008982105180621147\n",
            "epoch: 22/100, step: 250/250, loss: 0.008407584391534328\n",
            "epoch:21 loss:0.010\n",
            "epoch: 23/100, step: 50/250, loss: 0.009392345324158669\n",
            "epoch: 23/100, step: 100/250, loss: 0.007283081766217947\n",
            "epoch: 23/100, step: 150/250, loss: 0.009729878045618534\n",
            "epoch: 23/100, step: 200/250, loss: 0.007569211535155773\n",
            "epoch: 23/100, step: 250/250, loss: 0.012374725192785263\n",
            "epoch:22 loss:0.009\n",
            "epoch: 24/100, step: 50/250, loss: 0.010501767508685589\n",
            "epoch: 24/100, step: 100/250, loss: 0.008419542573392391\n",
            "epoch: 24/100, step: 150/250, loss: 0.008610512129962444\n",
            "epoch: 24/100, step: 200/250, loss: 0.009095176123082638\n",
            "epoch: 24/100, step: 250/250, loss: 0.008091019466519356\n",
            "epoch:23 loss:0.009\n",
            "epoch: 25/100, step: 50/250, loss: 0.0074206325225532055\n",
            "epoch: 25/100, step: 100/250, loss: 0.008114020340144634\n",
            "epoch: 25/100, step: 150/250, loss: 0.007406436838209629\n",
            "epoch: 25/100, step: 200/250, loss: 0.007006424479186535\n",
            "epoch: 25/100, step: 250/250, loss: 0.006887407042086124\n",
            "epoch:24 loss:0.009\n",
            "epoch: 26/100, step: 50/250, loss: 0.008103618398308754\n",
            "epoch: 26/100, step: 100/250, loss: 0.00715760700404644\n",
            "epoch: 26/100, step: 150/250, loss: 0.009785082191228867\n",
            "epoch: 26/100, step: 200/250, loss: 0.007267316337674856\n",
            "epoch: 26/100, step: 250/250, loss: 0.0093366215005517\n",
            "epoch:25 loss:0.008\n",
            "epoch: 27/100, step: 50/250, loss: 0.007987909018993378\n",
            "epoch: 27/100, step: 100/250, loss: 0.009610493667423725\n",
            "epoch: 27/100, step: 150/250, loss: 0.008034087717533112\n",
            "epoch: 27/100, step: 200/250, loss: 0.00624652998521924\n",
            "epoch: 27/100, step: 250/250, loss: 0.006198779679834843\n",
            "epoch:26 loss:0.008\n",
            "epoch: 28/100, step: 50/250, loss: 0.007465552072972059\n",
            "epoch: 28/100, step: 100/250, loss: 0.00785050168633461\n",
            "epoch: 28/100, step: 150/250, loss: 0.007127346470952034\n",
            "epoch: 28/100, step: 200/250, loss: 0.007851796224713326\n",
            "epoch: 28/100, step: 250/250, loss: 0.006622340530157089\n",
            "epoch:27 loss:0.007\n",
            "epoch: 29/100, step: 50/250, loss: 0.006935495883226395\n",
            "epoch: 29/100, step: 100/250, loss: 0.005723041947931051\n",
            "epoch: 29/100, step: 150/250, loss: 0.005469074472784996\n",
            "epoch: 29/100, step: 200/250, loss: 0.006735938601195812\n",
            "epoch: 29/100, step: 250/250, loss: 0.007781574502587318\n",
            "epoch:28 loss:0.007\n",
            "epoch: 30/100, step: 50/250, loss: 0.006007417570799589\n",
            "epoch: 30/100, step: 100/250, loss: 0.00437586335465312\n",
            "epoch: 30/100, step: 150/250, loss: 0.00704524852335453\n",
            "epoch: 30/100, step: 200/250, loss: 0.005468016490340233\n",
            "epoch: 30/100, step: 250/250, loss: 0.006357904057949781\n",
            "epoch:29 loss:0.007\n",
            "epoch: 31/100, step: 50/250, loss: 0.008204149082303047\n",
            "epoch: 31/100, step: 100/250, loss: 0.00738978898152709\n",
            "epoch: 31/100, step: 150/250, loss: 0.00588800897821784\n",
            "epoch: 31/100, step: 200/250, loss: 0.005113466177135706\n",
            "epoch: 31/100, step: 250/250, loss: 0.006597667001187801\n",
            "epoch:30 loss:0.006\n",
            "epoch: 32/100, step: 50/250, loss: 0.0066141774877905846\n",
            "epoch: 32/100, step: 100/250, loss: 0.006105078384280205\n",
            "epoch: 32/100, step: 150/250, loss: 0.006347890477627516\n",
            "epoch: 32/100, step: 200/250, loss: 0.006319615989923477\n",
            "epoch: 32/100, step: 250/250, loss: 0.004861331544816494\n",
            "epoch:31 loss:0.006\n",
            "epoch: 33/100, step: 50/250, loss: 0.0052759102545678616\n",
            "epoch: 33/100, step: 100/250, loss: 0.006042237393558025\n",
            "epoch: 33/100, step: 150/250, loss: 0.006264677736908197\n",
            "epoch: 33/100, step: 200/250, loss: 0.007084176409989595\n",
            "epoch: 33/100, step: 250/250, loss: 0.007203372661024332\n",
            "epoch:32 loss:0.006\n",
            "epoch: 34/100, step: 50/250, loss: 0.005528951995074749\n",
            "epoch: 34/100, step: 100/250, loss: 0.005793536081910133\n",
            "epoch: 34/100, step: 150/250, loss: 0.006155607756227255\n",
            "epoch: 34/100, step: 200/250, loss: 0.006267874967306852\n",
            "epoch: 34/100, step: 250/250, loss: 0.006311358883976936\n",
            "epoch:33 loss:0.006\n",
            "epoch: 35/100, step: 50/250, loss: 0.005186546593904495\n",
            "epoch: 35/100, step: 100/250, loss: 0.004919837694615126\n",
            "epoch: 35/100, step: 150/250, loss: 0.005536592565476894\n",
            "epoch: 35/100, step: 200/250, loss: 0.006220324896275997\n",
            "epoch: 35/100, step: 250/250, loss: 0.005456587765365839\n",
            "epoch:34 loss:0.006\n",
            "epoch: 36/100, step: 50/250, loss: 0.005595473572611809\n",
            "epoch: 36/100, step: 100/250, loss: 0.004765270743519068\n",
            "epoch: 36/100, step: 150/250, loss: 0.004258652683347464\n",
            "epoch: 36/100, step: 200/250, loss: 0.004525099880993366\n",
            "epoch: 36/100, step: 250/250, loss: 0.0047793579287827015\n",
            "epoch:35 loss:0.005\n",
            "epoch: 37/100, step: 50/250, loss: 0.004119521472603083\n",
            "epoch: 37/100, step: 100/250, loss: 0.004577294457703829\n",
            "epoch: 37/100, step: 150/250, loss: 0.004314683377742767\n",
            "epoch: 37/100, step: 200/250, loss: 0.004818288143724203\n",
            "epoch: 37/100, step: 250/250, loss: 0.004436564166098833\n",
            "epoch:36 loss:0.005\n",
            "epoch: 38/100, step: 50/250, loss: 0.005861989688128233\n",
            "epoch: 38/100, step: 100/250, loss: 0.0053343637846410275\n",
            "epoch: 38/100, step: 150/250, loss: 0.0056395987048745155\n",
            "epoch: 38/100, step: 200/250, loss: 0.004753584507852793\n",
            "epoch: 38/100, step: 250/250, loss: 0.005128654185682535\n",
            "epoch:37 loss:0.005\n",
            "epoch: 39/100, step: 50/250, loss: 0.0043184938840568066\n",
            "epoch: 39/100, step: 100/250, loss: 0.0033523638267070055\n",
            "epoch: 39/100, step: 150/250, loss: 0.005249235779047012\n",
            "epoch: 39/100, step: 200/250, loss: 0.005044302903115749\n",
            "epoch: 39/100, step: 250/250, loss: 0.005425565876066685\n",
            "epoch:38 loss:0.005\n",
            "epoch: 40/100, step: 50/250, loss: 0.003912271000444889\n",
            "epoch: 40/100, step: 100/250, loss: 0.004652899689972401\n",
            "epoch: 40/100, step: 150/250, loss: 0.004693895578384399\n",
            "epoch: 40/100, step: 200/250, loss: 0.005361311137676239\n",
            "epoch: 40/100, step: 250/250, loss: 0.004529987927526236\n",
            "epoch:39 loss:0.005\n",
            "epoch: 41/100, step: 50/250, loss: 0.004335915204137564\n",
            "epoch: 41/100, step: 100/250, loss: 0.006421165075153112\n",
            "epoch: 41/100, step: 150/250, loss: 0.004424053244292736\n",
            "epoch: 41/100, step: 200/250, loss: 0.004052925389260054\n",
            "epoch: 41/100, step: 250/250, loss: 0.004239252768456936\n",
            "epoch:40 loss:0.005\n",
            "epoch: 42/100, step: 50/250, loss: 0.004088944755494595\n",
            "epoch: 42/100, step: 100/250, loss: 0.00334971328265965\n",
            "epoch: 42/100, step: 150/250, loss: 0.004446143284440041\n",
            "epoch: 42/100, step: 200/250, loss: 0.0038536223582923412\n",
            "epoch: 42/100, step: 250/250, loss: 0.004885378759354353\n",
            "epoch:41 loss:0.004\n",
            "epoch: 43/100, step: 50/250, loss: 0.003607429563999176\n",
            "epoch: 43/100, step: 100/250, loss: 0.0034125109668821096\n",
            "epoch: 43/100, step: 150/250, loss: 0.0037618977949023247\n",
            "epoch: 43/100, step: 200/250, loss: 0.004273532889783382\n",
            "epoch: 43/100, step: 250/250, loss: 0.004083344712853432\n",
            "epoch:42 loss:0.004\n",
            "epoch: 44/100, step: 50/250, loss: 0.003444958245381713\n",
            "epoch: 44/100, step: 100/250, loss: 0.004277482628822327\n",
            "epoch: 44/100, step: 150/250, loss: 0.003197646699845791\n",
            "epoch: 44/100, step: 200/250, loss: 0.0051798163913190365\n",
            "epoch: 44/100, step: 250/250, loss: 0.003911117557436228\n",
            "epoch:43 loss:0.004\n",
            "epoch: 45/100, step: 50/250, loss: 0.004086803644895554\n",
            "epoch: 45/100, step: 100/250, loss: 0.0035220119170844555\n",
            "epoch: 45/100, step: 150/250, loss: 0.0038540195673704147\n",
            "epoch: 45/100, step: 200/250, loss: 0.0034819156862795353\n",
            "epoch: 45/100, step: 250/250, loss: 0.005289486609399319\n",
            "epoch:44 loss:0.004\n",
            "epoch: 46/100, step: 50/250, loss: 0.00347075704485178\n",
            "epoch: 46/100, step: 100/250, loss: 0.006100900005549192\n",
            "epoch: 46/100, step: 150/250, loss: 0.003407838521525264\n",
            "epoch: 46/100, step: 200/250, loss: 0.0035698111169040203\n",
            "epoch: 46/100, step: 250/250, loss: 0.003986566327512264\n",
            "epoch:45 loss:0.004\n",
            "epoch: 47/100, step: 50/250, loss: 0.005122114904224873\n",
            "epoch: 47/100, step: 100/250, loss: 0.0028585088439285755\n",
            "epoch: 47/100, step: 150/250, loss: 0.0039903889410197735\n",
            "epoch: 47/100, step: 200/250, loss: 0.0035541742108762264\n",
            "epoch: 47/100, step: 250/250, loss: 0.003472051350399852\n",
            "epoch:46 loss:0.004\n",
            "epoch: 48/100, step: 50/250, loss: 0.004320540931075811\n",
            "epoch: 48/100, step: 100/250, loss: 0.0076003470458090305\n",
            "epoch: 48/100, step: 150/250, loss: 0.004073938354849815\n",
            "epoch: 48/100, step: 200/250, loss: 0.0036914965603500605\n",
            "epoch: 48/100, step: 250/250, loss: 0.0031373347155749798\n",
            "epoch:47 loss:0.004\n",
            "epoch: 49/100, step: 50/250, loss: 0.0039182184264063835\n",
            "epoch: 49/100, step: 100/250, loss: 0.00348014198243618\n",
            "epoch: 49/100, step: 150/250, loss: 0.005117072258144617\n",
            "epoch: 49/100, step: 200/250, loss: 0.004509327933192253\n",
            "epoch: 49/100, step: 250/250, loss: 0.005050463601946831\n",
            "epoch:48 loss:0.003\n",
            "epoch: 50/100, step: 50/250, loss: 0.0037217349745333195\n",
            "epoch: 50/100, step: 100/250, loss: 0.0034137829206883907\n",
            "epoch: 50/100, step: 150/250, loss: 0.0037221605889499187\n",
            "epoch: 50/100, step: 200/250, loss: 0.00357087142765522\n",
            "epoch: 50/100, step: 250/250, loss: 0.0033399956300854683\n",
            "epoch:49 loss:0.003\n",
            "epoch: 51/100, step: 50/250, loss: 0.0034158646594733\n",
            "epoch: 51/100, step: 100/250, loss: 0.005235146265476942\n",
            "epoch: 51/100, step: 150/250, loss: 0.003307649400085211\n",
            "epoch: 51/100, step: 200/250, loss: 0.003668304067105055\n",
            "epoch: 51/100, step: 250/250, loss: 0.0028869963716715574\n",
            "epoch:50 loss:0.004\n",
            "epoch: 52/100, step: 50/250, loss: 0.002971051027998328\n",
            "epoch: 52/100, step: 100/250, loss: 0.003588341409340501\n",
            "epoch: 52/100, step: 150/250, loss: 0.0033832495100796223\n",
            "epoch: 52/100, step: 200/250, loss: 0.0033258304465562105\n",
            "epoch: 52/100, step: 250/250, loss: 0.003360379720106721\n",
            "epoch:51 loss:0.003\n",
            "epoch: 53/100, step: 50/250, loss: 0.0030467857141047716\n",
            "epoch: 53/100, step: 100/250, loss: 0.003466628957539797\n",
            "epoch: 53/100, step: 150/250, loss: 0.0031297607347369194\n",
            "epoch: 53/100, step: 200/250, loss: 0.0029320111498236656\n",
            "epoch: 53/100, step: 250/250, loss: 0.003338470822200179\n",
            "epoch:52 loss:0.003\n",
            "epoch: 54/100, step: 50/250, loss: 0.003193533280864358\n",
            "epoch: 54/100, step: 100/250, loss: 0.0028676502406597137\n",
            "epoch: 54/100, step: 150/250, loss: 0.003284371690824628\n",
            "epoch: 54/100, step: 200/250, loss: 0.0028847523499280214\n",
            "epoch: 54/100, step: 250/250, loss: 0.002667848253622651\n",
            "epoch:53 loss:0.003\n",
            "epoch: 55/100, step: 50/250, loss: 0.002690812572836876\n",
            "epoch: 55/100, step: 100/250, loss: 0.0026906407438218594\n",
            "epoch: 55/100, step: 150/250, loss: 0.0030937723349779844\n",
            "epoch: 55/100, step: 200/250, loss: 0.0037699451204389334\n",
            "epoch: 55/100, step: 250/250, loss: 0.002875406062230468\n",
            "epoch:54 loss:0.003\n",
            "epoch: 56/100, step: 50/250, loss: 0.0033323182724416256\n",
            "epoch: 56/100, step: 100/250, loss: 0.00328067340888083\n",
            "epoch: 56/100, step: 150/250, loss: 0.0032516864594072104\n",
            "epoch: 56/100, step: 200/250, loss: 0.002402959857136011\n",
            "epoch: 56/100, step: 250/250, loss: 0.0024524754844605923\n",
            "epoch:55 loss:0.003\n",
            "epoch: 57/100, step: 50/250, loss: 0.002812891500070691\n",
            "epoch: 57/100, step: 100/250, loss: 0.002873881720006466\n",
            "epoch: 57/100, step: 150/250, loss: 0.003479371313005686\n",
            "epoch: 57/100, step: 200/250, loss: 0.0034576603211462498\n",
            "epoch: 57/100, step: 250/250, loss: 0.002992833498865366\n",
            "epoch:56 loss:0.003\n",
            "epoch: 58/100, step: 50/250, loss: 0.0028600150253623724\n",
            "epoch: 58/100, step: 100/250, loss: 0.003056853311136365\n",
            "epoch: 58/100, step: 150/250, loss: 0.0030043525621294975\n",
            "epoch: 58/100, step: 200/250, loss: 0.0030397011432796717\n",
            "epoch: 58/100, step: 250/250, loss: 0.0036789914593100548\n",
            "epoch:57 loss:0.003\n",
            "epoch: 59/100, step: 50/250, loss: 0.0025865829084068537\n",
            "epoch: 59/100, step: 100/250, loss: 0.003048500744625926\n",
            "epoch: 59/100, step: 150/250, loss: 0.004528992809355259\n",
            "epoch: 59/100, step: 200/250, loss: 0.002577066421508789\n",
            "epoch: 59/100, step: 250/250, loss: 0.0029943257104605436\n",
            "epoch:58 loss:0.003\n",
            "epoch: 60/100, step: 50/250, loss: 0.0031359612476080656\n",
            "epoch: 60/100, step: 100/250, loss: 0.002856566570699215\n",
            "epoch: 60/100, step: 150/250, loss: 0.0025606409180909395\n",
            "epoch: 60/100, step: 200/250, loss: 0.0029165102168917656\n",
            "epoch: 60/100, step: 250/250, loss: 0.0031206258572638035\n",
            "epoch:59 loss:0.003\n",
            "epoch: 61/100, step: 50/250, loss: 0.0032468135468661785\n",
            "epoch: 61/100, step: 100/250, loss: 0.003722334047779441\n",
            "epoch: 61/100, step: 150/250, loss: 0.0035665342584252357\n",
            "epoch: 61/100, step: 200/250, loss: 0.003508790396153927\n",
            "epoch: 61/100, step: 250/250, loss: 0.0028614557813853025\n",
            "epoch:60 loss:0.003\n",
            "epoch: 62/100, step: 50/250, loss: 0.0027833967469632626\n",
            "epoch: 62/100, step: 100/250, loss: 0.002651078160852194\n",
            "epoch: 62/100, step: 150/250, loss: 0.0031115016900002956\n",
            "epoch: 62/100, step: 200/250, loss: 0.0031597004272043705\n",
            "epoch: 62/100, step: 250/250, loss: 0.0029822273645550013\n",
            "epoch:61 loss:0.003\n",
            "epoch: 63/100, step: 50/250, loss: 0.0027896035462617874\n",
            "epoch: 63/100, step: 100/250, loss: 0.002689740853384137\n",
            "epoch: 63/100, step: 150/250, loss: 0.002713362220674753\n",
            "epoch: 63/100, step: 200/250, loss: 0.002791247796267271\n",
            "epoch: 63/100, step: 250/250, loss: 0.002749609062448144\n",
            "epoch:62 loss:0.003\n",
            "epoch: 64/100, step: 50/250, loss: 0.003893462475389242\n",
            "epoch: 64/100, step: 100/250, loss: 0.0023661181330680847\n",
            "epoch: 64/100, step: 150/250, loss: 0.0023675374686717987\n",
            "epoch: 64/100, step: 200/250, loss: 0.0027437403332442045\n",
            "epoch: 64/100, step: 250/250, loss: 0.002341741696000099\n",
            "epoch:63 loss:0.003\n",
            "epoch: 65/100, step: 50/250, loss: 0.0026351874694228172\n",
            "epoch: 65/100, step: 100/250, loss: 0.00344450562261045\n",
            "epoch: 65/100, step: 150/250, loss: 0.0034407046623528004\n",
            "epoch: 65/100, step: 200/250, loss: 0.0023587888572365046\n",
            "epoch: 65/100, step: 250/250, loss: 0.002696160925552249\n",
            "epoch:64 loss:0.003\n",
            "epoch: 66/100, step: 50/250, loss: 0.0022389166988432407\n",
            "epoch: 66/100, step: 100/250, loss: 0.00257651275023818\n",
            "epoch: 66/100, step: 150/250, loss: 0.002148496685549617\n",
            "epoch: 66/100, step: 200/250, loss: 0.002348569920286536\n",
            "epoch: 66/100, step: 250/250, loss: 0.0028940325137227774\n",
            "epoch:65 loss:0.003\n",
            "epoch: 67/100, step: 50/250, loss: 0.0041156355291605\n",
            "epoch: 67/100, step: 100/250, loss: 0.002086385851725936\n",
            "epoch: 67/100, step: 150/250, loss: 0.0023590303026139736\n",
            "epoch: 67/100, step: 200/250, loss: 0.00254138489253819\n",
            "epoch: 67/100, step: 250/250, loss: 0.0025124826934188604\n",
            "epoch:66 loss:0.003\n",
            "epoch: 68/100, step: 50/250, loss: 0.004153653047978878\n",
            "epoch: 68/100, step: 100/250, loss: 0.002851582830771804\n",
            "epoch: 68/100, step: 150/250, loss: 0.0032670251093804836\n",
            "epoch: 68/100, step: 200/250, loss: 0.002710612490773201\n",
            "epoch: 68/100, step: 250/250, loss: 0.003155452897772193\n",
            "epoch:67 loss:0.003\n",
            "epoch: 69/100, step: 50/250, loss: 0.002936621895059943\n",
            "epoch: 69/100, step: 100/250, loss: 0.003521716920658946\n",
            "epoch: 69/100, step: 150/250, loss: 0.002989694243296981\n",
            "epoch: 69/100, step: 200/250, loss: 0.0027227208483964205\n",
            "epoch: 69/100, step: 250/250, loss: 0.0026704221963882446\n",
            "epoch:68 loss:0.003\n",
            "epoch: 70/100, step: 50/250, loss: 0.003715042257681489\n",
            "epoch: 70/100, step: 100/250, loss: 0.0031569842249155045\n",
            "epoch: 70/100, step: 150/250, loss: 0.0027140656020492315\n",
            "epoch: 70/100, step: 200/250, loss: 0.0026174208614975214\n",
            "epoch: 70/100, step: 250/250, loss: 0.002815127605572343\n",
            "epoch:69 loss:0.003\n",
            "epoch: 71/100, step: 50/250, loss: 0.0024625123478472233\n",
            "epoch: 71/100, step: 100/250, loss: 0.0026624277234077454\n",
            "epoch: 71/100, step: 150/250, loss: 0.0024452193174511194\n",
            "epoch: 71/100, step: 200/250, loss: 0.0028425443451851606\n",
            "epoch: 71/100, step: 250/250, loss: 0.0030506723560392857\n",
            "epoch:70 loss:0.003\n",
            "epoch: 72/100, step: 50/250, loss: 0.003003139514476061\n",
            "epoch: 72/100, step: 100/250, loss: 0.0032699457369744778\n",
            "epoch: 72/100, step: 150/250, loss: 0.002444547601044178\n",
            "epoch: 72/100, step: 200/250, loss: 0.0030520260334014893\n",
            "epoch: 72/100, step: 250/250, loss: 0.002465864410623908\n",
            "epoch:71 loss:0.003\n",
            "epoch: 73/100, step: 50/250, loss: 0.0027105507906526327\n",
            "epoch: 73/100, step: 100/250, loss: 0.0023817261680960655\n",
            "epoch: 73/100, step: 150/250, loss: 0.002438152674585581\n",
            "epoch: 73/100, step: 200/250, loss: 0.0022352247033268213\n",
            "epoch: 73/100, step: 250/250, loss: 0.0023202949669212103\n",
            "epoch:72 loss:0.003\n",
            "epoch: 74/100, step: 50/250, loss: 0.0026589413173496723\n",
            "epoch: 74/100, step: 100/250, loss: 0.0023200882133096457\n",
            "epoch: 74/100, step: 150/250, loss: 0.002530300058424473\n",
            "epoch: 74/100, step: 200/250, loss: 0.0026949350722134113\n",
            "epoch: 74/100, step: 250/250, loss: 0.002284492366015911\n",
            "epoch:73 loss:0.002\n",
            "epoch: 75/100, step: 50/250, loss: 0.002498902380466461\n",
            "epoch: 75/100, step: 100/250, loss: 0.0022776774130761623\n",
            "epoch: 75/100, step: 150/250, loss: 0.002483982592821121\n",
            "epoch: 75/100, step: 200/250, loss: 0.0032908900175243616\n",
            "epoch: 75/100, step: 250/250, loss: 0.00210774433799088\n",
            "epoch:74 loss:0.002\n",
            "epoch: 76/100, step: 50/250, loss: 0.0027263513766229153\n",
            "epoch: 76/100, step: 100/250, loss: 0.0027785419952124357\n",
            "epoch: 76/100, step: 150/250, loss: 0.0021513577084988356\n",
            "epoch: 76/100, step: 200/250, loss: 0.0025984698440879583\n",
            "epoch: 76/100, step: 250/250, loss: 0.002556076506152749\n",
            "epoch:75 loss:0.002\n",
            "epoch: 77/100, step: 50/250, loss: 0.0026193975936621428\n",
            "epoch: 77/100, step: 100/250, loss: 0.0028715378139168024\n",
            "epoch: 77/100, step: 150/250, loss: 0.002615608973428607\n",
            "epoch: 77/100, step: 200/250, loss: 0.0027257746551185846\n",
            "epoch: 77/100, step: 250/250, loss: 0.0024902180302888155\n",
            "epoch:76 loss:0.003\n",
            "epoch: 78/100, step: 50/250, loss: 0.0024590715765953064\n",
            "epoch: 78/100, step: 100/250, loss: 0.003317645750939846\n",
            "epoch: 78/100, step: 150/250, loss: 0.0037219799123704433\n",
            "epoch: 78/100, step: 200/250, loss: 0.002448305720463395\n",
            "epoch: 78/100, step: 250/250, loss: 0.0028373710811138153\n",
            "epoch:77 loss:0.003\n",
            "epoch: 79/100, step: 50/250, loss: 0.0028304397128522396\n",
            "epoch: 79/100, step: 100/250, loss: 0.0028316471725702286\n",
            "epoch: 79/100, step: 150/250, loss: 0.005042738281190395\n",
            "epoch: 79/100, step: 200/250, loss: 0.00260717049241066\n",
            "epoch: 79/100, step: 250/250, loss: 0.0030906409956514835\n",
            "epoch:78 loss:0.003\n",
            "epoch: 80/100, step: 50/250, loss: 0.0023470779415220022\n",
            "epoch: 80/100, step: 100/250, loss: 0.0023649439681321383\n",
            "epoch: 80/100, step: 150/250, loss: 0.0023274850100278854\n",
            "epoch: 80/100, step: 200/250, loss: 0.0028409480582922697\n",
            "epoch: 80/100, step: 250/250, loss: 0.003011109307408333\n",
            "epoch:79 loss:0.003\n",
            "epoch: 81/100, step: 50/250, loss: 0.003919149283319712\n",
            "epoch: 81/100, step: 100/250, loss: 0.00268261949531734\n",
            "epoch: 81/100, step: 150/250, loss: 0.0024222908541560173\n",
            "epoch: 81/100, step: 200/250, loss: 0.002968911547213793\n",
            "epoch: 81/100, step: 250/250, loss: 0.0031711196061223745\n",
            "epoch:80 loss:0.003\n",
            "epoch: 82/100, step: 50/250, loss: 0.002462959848344326\n",
            "epoch: 82/100, step: 100/250, loss: 0.002048908732831478\n",
            "epoch: 82/100, step: 150/250, loss: 0.0023697807919234037\n",
            "epoch: 82/100, step: 200/250, loss: 0.0024223013315349817\n",
            "epoch: 82/100, step: 250/250, loss: 0.002321498468518257\n",
            "epoch:81 loss:0.002\n",
            "epoch: 83/100, step: 50/250, loss: 0.004042297136038542\n",
            "epoch: 83/100, step: 100/250, loss: 0.0023324505891650915\n",
            "epoch: 83/100, step: 150/250, loss: 0.0030293164309114218\n",
            "epoch: 83/100, step: 200/250, loss: 0.0021143213380128145\n",
            "epoch: 83/100, step: 250/250, loss: 0.0021397199016064405\n",
            "epoch:82 loss:0.002\n",
            "epoch: 84/100, step: 50/250, loss: 0.0020786139648407698\n",
            "epoch: 84/100, step: 100/250, loss: 0.00201793503947556\n",
            "epoch: 84/100, step: 150/250, loss: 0.002234350424259901\n",
            "epoch: 84/100, step: 200/250, loss: 0.002014709636569023\n",
            "epoch: 84/100, step: 250/250, loss: 0.0020398094784468412\n",
            "epoch:83 loss:0.002\n",
            "epoch: 85/100, step: 50/250, loss: 0.0023843387607485056\n",
            "epoch: 85/100, step: 100/250, loss: 0.0022855934221297503\n",
            "epoch: 85/100, step: 150/250, loss: 0.002106912899762392\n",
            "epoch: 85/100, step: 200/250, loss: 0.0020332755520939827\n",
            "epoch: 85/100, step: 250/250, loss: 0.002160215750336647\n",
            "epoch:84 loss:0.002\n",
            "epoch: 86/100, step: 50/250, loss: 0.002944681327790022\n",
            "epoch: 86/100, step: 100/250, loss: 0.0022060745395720005\n",
            "epoch: 86/100, step: 150/250, loss: 0.002767144236713648\n",
            "epoch: 86/100, step: 200/250, loss: 0.0031796484254300594\n",
            "epoch: 86/100, step: 250/250, loss: 0.0026414673775434494\n",
            "epoch:85 loss:0.003\n",
            "epoch: 87/100, step: 50/250, loss: 0.0025845973286777735\n",
            "epoch: 87/100, step: 100/250, loss: 0.002787934383377433\n",
            "epoch: 87/100, step: 150/250, loss: 0.0025557565968483686\n",
            "epoch: 87/100, step: 200/250, loss: 0.0034197475761175156\n",
            "epoch: 87/100, step: 250/250, loss: 0.0028119005728513002\n",
            "epoch:86 loss:0.003\n",
            "epoch: 88/100, step: 50/250, loss: 0.002401673700660467\n",
            "epoch: 88/100, step: 100/250, loss: 0.0022530474234372377\n",
            "epoch: 88/100, step: 150/250, loss: 0.002382447011768818\n",
            "epoch: 88/100, step: 200/250, loss: 0.003096875036135316\n",
            "epoch: 88/100, step: 250/250, loss: 0.0026946826837956905\n",
            "epoch:87 loss:0.003\n",
            "epoch: 89/100, step: 50/250, loss: 0.002755219815298915\n",
            "epoch: 89/100, step: 100/250, loss: 0.0025672370102256536\n",
            "epoch: 89/100, step: 150/250, loss: 0.0028597498312592506\n",
            "epoch: 89/100, step: 200/250, loss: 0.0028808265924453735\n",
            "epoch: 89/100, step: 250/250, loss: 0.002257258864119649\n",
            "epoch:88 loss:0.003\n",
            "epoch: 90/100, step: 50/250, loss: 0.002875613747164607\n",
            "epoch: 90/100, step: 100/250, loss: 0.002713406691327691\n",
            "epoch: 90/100, step: 150/250, loss: 0.0028159269131720066\n",
            "epoch: 90/100, step: 200/250, loss: 0.003739377250894904\n",
            "epoch: 90/100, step: 250/250, loss: 0.002621585037559271\n",
            "epoch:89 loss:0.003\n",
            "epoch: 91/100, step: 50/250, loss: 0.0021495020482689142\n",
            "epoch: 91/100, step: 100/250, loss: 0.002357348334044218\n",
            "epoch: 91/100, step: 150/250, loss: 0.002262612571939826\n",
            "epoch: 91/100, step: 200/250, loss: 0.0025523193180561066\n",
            "epoch: 91/100, step: 250/250, loss: 0.00224179495126009\n",
            "epoch:90 loss:0.003\n",
            "epoch: 92/100, step: 50/250, loss: 0.002308788476511836\n",
            "epoch: 92/100, step: 100/250, loss: 0.002659178338944912\n",
            "epoch: 92/100, step: 150/250, loss: 0.0022129621356725693\n",
            "epoch: 92/100, step: 200/250, loss: 0.0021740030497312546\n",
            "epoch: 92/100, step: 250/250, loss: 0.0021070342045277357\n",
            "epoch:91 loss:0.002\n",
            "epoch: 93/100, step: 50/250, loss: 0.0033898891415446997\n",
            "epoch: 93/100, step: 100/250, loss: 0.002016510581597686\n",
            "epoch: 93/100, step: 150/250, loss: 0.0025141024962067604\n",
            "epoch: 93/100, step: 200/250, loss: 0.00212364224717021\n",
            "epoch: 93/100, step: 250/250, loss: 0.002066604793071747\n",
            "epoch:92 loss:0.002\n",
            "epoch: 94/100, step: 50/250, loss: 0.001959966728463769\n",
            "epoch: 94/100, step: 100/250, loss: 0.002035026904195547\n",
            "epoch: 94/100, step: 150/250, loss: 0.002119912998750806\n",
            "epoch: 94/100, step: 200/250, loss: 0.0021010737400501966\n",
            "epoch: 94/100, step: 250/250, loss: 0.0021007517352700233\n",
            "epoch:93 loss:0.002\n",
            "epoch: 95/100, step: 50/250, loss: 0.0022771668154746294\n",
            "epoch: 95/100, step: 100/250, loss: 0.0019792267121374607\n",
            "epoch: 95/100, step: 150/250, loss: 0.002166502643376589\n",
            "epoch: 95/100, step: 200/250, loss: 0.002302109729498625\n",
            "epoch: 95/100, step: 250/250, loss: 0.002125244587659836\n",
            "epoch:94 loss:0.002\n",
            "epoch: 96/100, step: 50/250, loss: 0.002015882171690464\n",
            "epoch: 96/100, step: 100/250, loss: 0.002080255188047886\n",
            "epoch: 96/100, step: 150/250, loss: 0.0021665862295776606\n",
            "epoch: 96/100, step: 200/250, loss: 0.0023106939624994993\n",
            "epoch: 96/100, step: 250/250, loss: 0.0023961984552443027\n",
            "epoch:95 loss:0.002\n",
            "epoch: 97/100, step: 50/250, loss: 0.0020069184247404337\n",
            "epoch: 97/100, step: 100/250, loss: 0.0022716859821230173\n",
            "epoch: 97/100, step: 150/250, loss: 0.0033268521074205637\n",
            "epoch: 97/100, step: 200/250, loss: 0.0022212937474250793\n",
            "epoch: 97/100, step: 250/250, loss: 0.0024768863804638386\n",
            "epoch:96 loss:0.002\n",
            "epoch: 98/100, step: 50/250, loss: 0.0030245650559663773\n",
            "epoch: 98/100, step: 100/250, loss: 0.0020786053501069546\n",
            "epoch: 98/100, step: 150/250, loss: 0.0030449084006249905\n",
            "epoch: 98/100, step: 200/250, loss: 0.003291509347036481\n",
            "epoch: 98/100, step: 250/250, loss: 0.002532012527808547\n",
            "epoch:97 loss:0.003\n",
            "epoch: 99/100, step: 50/250, loss: 0.004207346588373184\n",
            "epoch: 99/100, step: 100/250, loss: 0.0021152994595468044\n",
            "epoch: 99/100, step: 150/250, loss: 0.0028824161272495985\n",
            "epoch: 99/100, step: 200/250, loss: 0.0033065532334148884\n",
            "epoch: 99/100, step: 250/250, loss: 0.0028623638208955526\n",
            "epoch:98 loss:0.003\n",
            "epoch: 100/100, step: 50/250, loss: 0.002896664896979928\n",
            "epoch: 100/100, step: 100/250, loss: 0.0029046484269201756\n",
            "epoch: 100/100, step: 150/250, loss: 0.003132706042379141\n",
            "epoch: 100/100, step: 200/250, loss: 0.0028911021072417498\n",
            "epoch: 100/100, step: 250/250, loss: 0.004257952328771353\n",
            "epoch:99 loss:0.003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-19T15:00:07.769974Z",
          "iopub.status.idle": "2021-07-19T15:00:07.770667Z"
        },
        "trusted": true,
        "id": "MtB6vfdk4dhN"
      },
      "source": [
        "# for batch_number, (X, y) in enumerate(dataloader):\n",
        "#     print(batch_number, X.shape, y.shape, tag_embeddings.shape)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-20T16:30:03.299217Z",
          "iopub.execute_input": "2021-07-20T16:30:03.2996Z",
          "iopub.status.idle": "2021-07-20T16:30:03.370014Z",
          "shell.execute_reply.started": "2021-07-20T16:30:03.299566Z",
          "shell.execute_reply": "2021-07-20T16:30:03.368847Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaKOwpDj4dhN",
        "outputId": "879f517a-a680-4f2f-963c-f3ac3932a1b4"
      },
      "source": [
        "# with torch.no_grad():\n",
        "#     result = calculate_metrics(np.array(model_result.cpu()), np.array(y.cpu()))\n",
        "#     print(result)\n",
        "with torch.no_grad():\n",
        "  model_result = []\n",
        "  targets = []\n",
        "  for X, y in train_dataloader:\n",
        "    X, y, tag_embeddings = X.to(device), y.to(device), tag_embeddings.to(device)\n",
        "    model_batch_result = model(X.long(), tag_embeddings)\n",
        "    model_result.extend(model_batch_result.cpu().numpy())\n",
        "    targets.extend(y.cpu().numpy())\n",
        "result = calculate_metrics(np.array(model_result), np.array(targets))\n",
        "print(result)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'micro/precision': 0.9930734309917162, 'micro/recall': 0.9386861313868613, 'micro/f1': 0.9651141635586281, 'macro/precision': 0.9438293329131467, 'macro/recall': 0.8717104990585849, 'macro/f1': 0.8994698317506449, 'samples/precision': 0.9888714781746032, 'samples/recall': 0.9460333333333333, 'samples/f1': 0.9607086198523698}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AcQXpGUmzCP",
        "outputId": "3a3a439e-5b69-4002-ed02-2f50cb917dd5"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model_result = []\n",
        "  targets = []\n",
        "  for X, y in val_dataloader:\n",
        "    X, y, tag_embeddings = X.to(device), y.to(device), tag_embeddings.to(device)\n",
        "    model_batch_result = model(X.long(), tag_embeddings)\n",
        "    model_result.extend(model_batch_result.cpu().numpy())\n",
        "    targets.extend(y.cpu().numpy())\n",
        "  result = calculate_metrics(np.array(model_result), np.array(targets))\n",
        "  print(result)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'micro/precision': 0.3699851411589896, 'micro/recall': 0.08749121574139143, 'micro/f1': 0.14151747655583977, 'macro/precision': 0.061947860492459324, 'macro/recall': 0.019415298521588745, 'macro/f1': 0.027183688286331435, 'samples/precision': 0.15826206016206015, 'samples/recall': 0.09771666666666667, 'samples/f1': 0.10688347208347207}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-19T15:00:07.773468Z",
          "iopub.status.idle": "2021-07-19T15:00:07.774165Z"
        },
        "trusted": true,
        "id": "NcriEzjY4dhN"
      },
      "source": [
        "# pred = np.array(model_result > 0.5, dtype=int)\n",
        "# print(f1_score(y_true=np.array(y), y_pred=pred, average = 'micro'))\n",
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-19T14:40:35.228783Z",
          "iopub.execute_input": "2021-07-19T14:40:35.229117Z",
          "iopub.status.idle": "2021-07-19T14:40:35.348724Z",
          "shell.execute_reply.started": "2021-07-19T14:40:35.229088Z",
          "shell.execute_reply": "2021-07-19T14:40:35.347178Z"
        },
        "trusted": true,
        "id": "KImxeUdc4dhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba00ad2-a38d-4f3a-a240-725dc5ad1ac8"
      },
      "source": [
        "x = torch.tensor([[1,2, 12,34, 56,78],\n",
        "                 [12,45, 99,67, 6, 54],\n",
        "                 [3,24, 6,99, 12,56]])\n",
        "embed = nn.Embedding(3325, 10, padding_idx=0)\n",
        "print(x.shape)\n",
        "x = embed(x)\n",
        "print(x.shape)\n",
        "# print(x[0])\n",
        "# x = x.permute([0, 2, 1])\n",
        "# print(x.shape)\n",
        "# conv1d = nn.Conv1d(50, 2048, 4, padding=0)\n",
        "# x = conv1d(x)\n",
        "# print(x.shape)\n",
        "# x = x.permute([0, 2, 1])\n",
        "# print(x.shape)\n",
        "# x, _  = torch.max(x, 1)\n",
        "# print(x.shape)\n",
        "# globalmaxpool1d = nn.AdaptiveMaxPool2d(output_size=(50, 1))\n",
        "# x = globalmaxpool1d(x)\n",
        "# print(x.shape)\n",
        "# x = x.squeeze()\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# gc1 = GraphConvolution(50, 1024)\n",
        "# y = gc1(torch.from_numpy(tag_embeddings), torch.from_numpy(A).float())\n",
        "# print(y.shape)\n",
        "# gc2 = GraphConvolution(1024, 2048)\n",
        "# y = gc2(y, torch.from_numpy(A).float())\n",
        "# print(y.shape)\n",
        "# y = y.transpose(0, 1)\n",
        "# print(y.shape)\n",
        "# y = torch.matmul(x, y)\n",
        "# print(y.shape)\n",
        "# sigm = nn.Sigmoid()\n",
        "# y = sigm(y)\n",
        "# print(y.shape)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 6])\n",
            "torch.Size([3, 6, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-19T14:40:11.307929Z",
          "iopub.execute_input": "2021-07-19T14:40:11.308511Z",
          "iopub.status.idle": "2021-07-19T14:40:11.504597Z",
          "shell.execute_reply.started": "2021-07-19T14:40:11.308463Z",
          "shell.execute_reply": "2021-07-19T14:40:11.503437Z"
        },
        "trusted": true,
        "id": "ii3xd6874dhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e184f2d6-3ac0-4037-a11f-e1a42dbe743c"
      },
      "source": [
        "with open(adj_matrix_path) as fp:\n",
        "    adj_data = json.load(fp)\n",
        "A = gen_A(723, t, p, adj_data)\n",
        "A.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(723, 723)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRrZLTsZ4dhO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "2dd0071f-96fc-491e-d6f8-11c6c90b6951"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-534b7a74019f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_XFUTl1In_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}